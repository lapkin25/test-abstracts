---
jupytext:
  cell_metadata_filter: -all
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.1
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Линейная регрессия

## Введение

В разделе [Метод наименьших квадратов](ls.md) рассмотрена задача линейной регрессии с точки зрения алгебры и оптимизации. Рассмотрим ту же задачу со статистической точки зрения.
{cite}`modernstat`
{cite}`introstat`

Напомним уравнение линейной регрессии (см. {cite}`modernstat`, с. 239):

$$
y_i = \alpha + \beta x_i + e_i, \quad i = 1, \ldots, n.
$$

Оно выражает линейную зависимость между двумя величинами: $x$ и $y$, которые заданы $n$ наблюдениями $(x_i, y_i)$, $i = 1, \ldots, n$. Числа $\alpha, \beta$ - это постоянные, а $e_i$ - это случайная составляющая с нулевым средним и постоянным среднеквадратичным разбросом (дисперсией). Коэффициент $\beta$ носит название *интерсепт*, $\alpha$ - *коэффициент наклона*.
Геометрически уравнение линейной регрессии изображается прямой линией, аппроксимирующей точки $(x_i, y_i)$.

Оценка регрессионных коэффициентов $\alpha, \beta$ производится с помощью метода наименьших квадратов. Находим оценки $a, b$ чисел $\alpha, \beta$, при которых минимальна *сумма квадратов отклонений*: (sum-of-squares error)

$$
\mathrm{SSE} = \sum_{i=1}^n (y_i - a - bx_i)^2.
$$

Если потребовать, чтобы линия проходила через центр $(\overline{x}, \overline{y})$, то

$$
\overline{y} = a+b\overline{x},
$$

отсюда

$$
a = \overline{y} - b\overline{x}.
$$

Подставляя данное выражение в формулу для SSE, придем к формуле

$$
\mathrm{SSE} = \sum_{i=1}^n (y_i - \overline{y} - b(x_i - \overline{x}))^2 =
$$
$$
= \sum_{i=1}^n (y_i - \overline{y})^2 - 2b\sum_{i=1}^n(x_i - \overline{x})(y_i - \overline{y}) + b^2\sum_{i=1}^n (x_i - \overline{x})^2.
$$

Если теперь разделить полученное равенство на $(n-1)$, то получим

$$
\frac{\mathrm{SSE}}{n-1} = S_y^2 - 2bS_{xy} + b^2S_x^2.
$$

Здесь $S_{xy}$ - выборочная ковариация, $S_x, S_y$ - выборочные дисперсии,

$$
S_{xy} = \frac{1}{n-1}\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y}),
$$
$$
S_{xx} = S_x, \quad S_{yy} = S_y.
$$

Теперь нужно выбрать коэффициент $b$ так, чтобы минимизировать данную величину.

Можно показать (см. {cite}`modernstat`, с. 240), что

$$
\frac{\mathrm{SSE}}{n-1} = S_y^2(1 - R_{xy}^2) + S_x^2\left(b - \frac{S_{xy}}{S_x^2}\right)^2.
$$

Здесь $R_{xy}$ - выборочный коэффициент корреляции:

$$
R_{xy} = \frac{S_{xy}}{S_x \cdot S_y}.
$$

Поэтому оптимальным значением $b$ будет то, при котором второе слагаемое обратится в ноль:

$$
b = \frac{S_{xy}}{S_{x}^2} = R_{xy}\frac{S_y}{S_x}.
$$

Средний квадрат ошибки $\mathrm{SSE}/(n-1)$ при оптимальных регрессионных коэффициентах равен

$$
S_{y|x}^2 = S_y^2(1 - R_{xy}^2).
$$

Величина

$$
R_{xy}^2 = 1 - \frac{S_{y|x}^2}{S_y^2}
$$

носит название *коэффициента детерминации*. Это число выражает, какую долю дисперсии величины $y$ "объясняет" линейная регрессия.

Также вводят скорректированный коэффициент детерминации:

$$
R_{xy}^2 \mathrm{(adjusted)} = 1 - \left[ (1-R_{xy}^2)\frac{n-1}{n-2} \right].
$$


